{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.sparse.linalg\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = codecs.open('parsed.json', 'r', encoding='utf-8')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f = codecs.open('win_loss_labels.json', 'r', encoding='utf-8')\n",
    "results = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f = codecs.open('parsed_testing.json', 'r', encoding='utf-8')\n",
    "test_data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 1399)\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "def tfidf_docterm(corpus, freqthresh):\n",
    "    \"\"\"Estimate document-term TF-IDF vectors for each document (line in filename),\n",
    "    where each column is a word, in decreasing order of frequency.\n",
    "    Ignore words that appear fewer than freqthresh times.\n",
    "    Return a list consisting of\n",
    "    1. a list of the m word types with at least freqthresh count, sorted in decreasing order of frequency.\n",
    "    2. an array with d rows and m columns,\n",
    "    where row i is the vector for the ith document in filename,\n",
    "    and col j represents the jth word in the above list.\n",
    "    \"\"\"\n",
    "    tfidf_dict = dict()\n",
    "    candidate_dict = {}\n",
    "    wordcounts = defaultdict(int)\n",
    "    new_corpus = []\n",
    "    labels = []\n",
    "    for candidate in corpus.keys(): \n",
    "        if candidate == 'Hillary Clinton republican 2008':\n",
    "            continue\n",
    "        labels.append(results[candidate])\n",
    "        debates = [debate for debates in corpus[candidate].values() for debate in debates]\n",
    "        for word in debates:\n",
    "                if word not in common:\n",
    "                    wordcounts[word] +=1\n",
    "        new_corpus.append([candidate, debates])\n",
    "    \n",
    "    sorted_words = sorted(filter(lambda x: wordcounts[x] >= freqthresh, wordcounts.keys()), key=lambda x: wordcounts[x], reverse=True)\n",
    "    thresholded_words = set(sorted_words)\n",
    "    word_indices = dict((word, index) for index, word in enumerate(sorted_words))\n",
    "    context = numpy.zeros((len(new_corpus), len(sorted_words)))\n",
    "    for di, doc in enumerate(new_corpus):\n",
    "        for word in doc[1]:\n",
    "            try:\n",
    "                #print word\n",
    "                if word in thresholded_words:\n",
    "                    context[di,word_indices[word]] +=1\n",
    "            except: \n",
    "                pass\n",
    "    return [sorted_words, context, labels]\n",
    "\n",
    "common = stopwords.words('english')\n",
    "\n",
    "tfidf_vectors = tfidf_docterm(data,50)\n",
    "vectorizer = tfidf_vectors[1]\n",
    "result = tfidf_vectors[2]\n",
    "print vectorizer.shape\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3.82670867   -1.56969255    0.95818609 ...,   -9.11066367\n",
      "    -2.13700424  -77.6101968 ]\n",
      " [   8.25540067  -11.83665582    3.33705647 ...,  -25.68261294\n",
      "   -15.38189016 -479.80194031]\n",
      " [  50.69721222   80.07391666   27.89746094 ...,  -70.80272083\n",
      "   -35.77168279 -512.9112537 ]\n",
      " ..., \n",
      " [  10.12864488   20.8345388    -4.57299857 ...,   13.95186028\n",
      "   -11.83080993 -191.22507628]\n",
      " [  71.3857324     7.78533655  -16.79635823 ...,   64.28246144\n",
      "    -3.29635289 -319.76082287]\n",
      " [  44.15076785   -5.71621032   -7.47670045 ...,   17.38807814   24.4171316\n",
      "  -298.50243687]] <class 'numpy.matrixlib.defmatrix.matrix'> (53, 20)\n"
     ]
    }
   ],
   "source": [
    "def dimensionality_reduce(vectors, ndims):\n",
    "    \"\"\"Apply SVD on original sparse matrix, return reduced vectors.\"\"\"\n",
    "    # Do not modify\n",
    "    U, s, Vh = scipy.sparse.linalg.svds(vectors, k=ndims)\n",
    "    sigmatrix = scipy.matrix(scipy.diag(s))\n",
    "    return U * sigmatrix\n",
    "\n",
    "train_data_features = dimensionality_reduce(vectorizer, 20)\n",
    "print train_data_features, type(train_data_features), train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 111.   87.  141. ...,    6.    0.    6.]\n",
      " [ 214.  117.   92. ...,    1.    1.    2.]\n",
      " [ 336.  147.  113. ...,    4.    3.    1.]\n",
      " ..., \n",
      " [ 526.  226.  269. ...,   23.    0.    4.]\n",
      " [ 575.  564.  360. ...,    6.   10.    6.]\n",
      " [   0.    0.    0. ...,    0.    0.    0.]]\n",
      "[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "test_tfidf_vectors = tfidf_docterm(test_data,100)\n",
    "test_vectorizer = test_tfidf_vectors[1]\n",
    "test_result = test_tfidf_vectors[2]\n",
    "print test_vectorizer\n",
    "print test_result\n",
    "test_data_features = dimensionality_reduce(test_vectorizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances calculated\n"
     ]
    }
   ],
   "source": [
    "def knn(trainpoints, traincats, testpoints, k):\n",
    "    \"\"\"Given training data points\n",
    "    and a 1-d array of the corresponding categories of the points,\n",
    "    predict category for each test point,\n",
    "    using k nearest neighbors (with cosine distance).\n",
    "    Return a 1-d array of predicted categories.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    distances = cdist(testpoints, trainpoints)\n",
    "    print 'distances calculated'\n",
    "    for test_dist in distances:\n",
    "        sorted_first_k = sorted(enumerate(test_dist), key = lambda e: e[1])[:k]\n",
    "        best_cat = ''\n",
    "        best_count = 0\n",
    "        cat_counts = defaultdict(int) \n",
    "        for ci, cat_dist in sorted_first_k:\n",
    "            cat_counts[traincats[ci]] += 1\n",
    "            if cat_counts[traincats[ci]] > best_count:\n",
    "                best_cat = traincats[ci]\n",
    "                best_count = cat_counts[traincats[ci]]\n",
    "        predictions.append(best_cat)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Marena\n",
    "def knn(trainpoints, traincats, testpoints, k):\n",
    "    \"\"\"Given training data points\n",
    "    and a 1-d array of the corresponding categories of the points,\n",
    "    predict category for each test point,\n",
    "    using k nearest neighbors (with cosine distance).\n",
    "    Return a 1-d array of predicted categories.\n",
    "    \"\"\"\n",
    "    cd = cdist(testpoints, trainpoints, 'cosine') # Creates array where i,j is distance between testpoints[i] and trainpoints[j]\n",
    "    sorted_dist = numpy.argsort(cd, axis=1)\n",
    "    testcats = []\n",
    "    for row in sorted_dist:\n",
    "        training_labels = defaultdict(lambda: 0)\n",
    "        nearest_neighbors = row[0:k]\n",
    "        for index in nearest_neighbors:\n",
    "            training_labels[traincats[index]] += 1\n",
    "        winning_label = max(training_labels.iteritems(), key=operator.itemgetter(1))[0]\n",
    "        testcats.append(winning_label)\n",
    "    return testcats\n",
    "\n",
    "test_predict_label = knn(train_data_features,result,test_data_features,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0.818181818182\n"
     ]
    }
   ],
   "source": [
    "print test_predict_label\n",
    "count = 0 \n",
    "for index,label in enumerate(test_predict_label):\n",
    "    if label == test_result[index]:\n",
    "        count+=1\n",
    "print count/33. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
